{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32293ea4-f0cd-47a2-80f8-22a99f76cd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.99      0.98      0.99       101\n",
      "entertainment       1.00      0.97      0.99        77\n",
      "     politics       0.98      0.98      0.98        84\n",
      "        sport       1.00      1.00      1.00       103\n",
      "         tech       0.96      1.00      0.98        80\n",
      "\n",
      "     accuracy                           0.99       445\n",
      "    macro avg       0.99      0.99      0.99       445\n",
      " weighted avg       0.99      0.99      0.99       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Путь к папке с данными\n",
    "data_path = r\"C:\\Users\\epg_F\\Homework_6\\bbc\"\n",
    "\n",
    "# Список категорий (названия папок)\n",
    "categories = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "\n",
    "# Создаем списки для хранения данных\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "# Функция для определения кодировки\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    return result[\"encoding\"]\n",
    "\n",
    "# Читаем файлы из каждой категории\n",
    "for category in categories:\n",
    "    category_path = os.path.join(data_path, category)\n",
    "    for filename in os.listdir(category_path):\n",
    "        file_path = os.path.join(category_path, filename)\n",
    "        encoding = detect_encoding(file_path)  # Определяем кодировку\n",
    "        with open(file_path, \"r\", encoding=encoding, errors=\"replace\") as file:\n",
    "            texts.append(file.read())\n",
    "            labels.append(category)\n",
    "\n",
    "# Создаем DataFrame\n",
    "df = pd.DataFrame({\"text\": texts, \"label\": labels})\n",
    "\n",
    "# Разделение на train/test\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"text\"], df[\"label\"], test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "# Функция предобработки текста\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z ]', '', text)  # Удаляем все, кроме букв и пробелов\n",
    "    text = text.lower().strip()  # Приводим к нижнему регистру\n",
    "    return text\n",
    "\n",
    "# Применяем предобработку\n",
    "train_texts = train_texts.apply(preprocess_text)\n",
    "test_texts = test_texts.apply(preprocess_text)\n",
    "\n",
    "# Создание и обучение SVM-модели с TF-IDF\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),  # Преобразуем текст в векторы\n",
    "    ('clf', SVC(probability=True))  # Классифицируем SVM\n",
    "])\n",
    "\n",
    "pipeline.fit(train_texts, train_labels)  # Обучаем модель\n",
    "predictions = pipeline.predict(test_texts)  # Делаем предсказания\n",
    "\n",
    "# Выводим метрики качества\n",
    "print(classification_report(test_labels, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcb41620-a0dc-4983-a24d-30bc2cb88450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label\n",
      "0  Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
      "1  Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
      "2  Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
      "3  High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
      "4  Pernod takeover talk lifts Domecq\\n\\nShares in...  business\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "# Путь к папке с данными\n",
    "data_path = r\"C:\\Users\\epg_F\\Homework_6\\bbc\"\n",
    "\n",
    "# Список категорий (названия папок)\n",
    "categories = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "\n",
    "# Создаем списки для хранения данных\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "# Функция для определения кодировки\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    return result[\"encoding\"]\n",
    "\n",
    "# Читаем файлы из каждой категории\n",
    "for category in categories:\n",
    "    category_path = os.path.join(data_path, category)\n",
    "    for filename in os.listdir(category_path):\n",
    "        file_path = os.path.join(category_path, filename)\n",
    "        encoding = detect_encoding(file_path)  # Определяем кодировку\n",
    "        with open(file_path, \"r\", encoding=encoding, errors=\"replace\") as file:\n",
    "            texts.append(file.read())\n",
    "            labels.append(category)\n",
    "\n",
    "# Создаем DataFrame\n",
    "df = pd.DataFrame({\"text\": texts, \"label\": labels})\n",
    "\n",
    "# Проверяем загруженные данные\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2f9fd83-0456-4040-8f6a-6f1e3508a177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"text\"], df[\"label\"], test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3b3acca-6610-4b74-97da-0fd32aa2dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z ]', '', text)  # Удаляем все, кроме букв и пробелов\n",
    "    text = text.lower().strip()  # Приводим к нижнему регистру\n",
    "    return text\n",
    "\n",
    "train_texts = train_texts.apply(preprocess_text)\n",
    "test_texts = test_texts.apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6de05e82-0f00-4f4f-a887-e97466a6a391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.99      0.98      0.99       101\n",
      "entertainment       1.00      0.97      0.99        77\n",
      "     politics       0.98      0.98      0.98        84\n",
      "        sport       1.00      1.00      1.00       103\n",
      "         tech       0.96      1.00      0.98        80\n",
      "\n",
      "     accuracy                           0.99       445\n",
      "    macro avg       0.99      0.99      0.99       445\n",
      " weighted avg       0.99      0.99      0.99       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),  # Преобразуем текст в векторы\n",
    "    ('clf', SVC(probability=True))  # Классифицируем SVM\n",
    "])\n",
    "\n",
    "pipeline.fit(train_texts, train_labels)  # Обучаем модель\n",
    "predictions = pipeline.predict(test_texts)  # Делаем предсказания\n",
    "\n",
    "print(classification_report(test_labels, predictions))  # Выводим метрики\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a3fac37-9c06-49e9-97d9-7af27b622c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32d72857e2d44c59697a3e505f737ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1776 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f05e0d3cd54eeb8180ec61bfc9fd6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/445 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\epg_F\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23/23 13:03, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.605500</td>\n",
       "      <td>1.594041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.38      0.33      0.35       101\n",
      "entertainment       0.06      0.04      0.05        77\n",
      "     politics       0.48      0.76      0.59        84\n",
      "        sport       0.23      0.39      0.29       103\n",
      "         tech       0.00      0.00      0.00        80\n",
      "\n",
      "     accuracy                           0.31       445\n",
      "    macro avg       0.23      0.30      0.25       445\n",
      " weighted avg       0.24      0.31      0.27       445\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\epg_F\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\epg_F\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\epg_F\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Проверяем доступность GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Кодируем лейблы в числовые значения\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Загружаем токенизатор\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Функция токенизации\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Создаем датасеты Hugging Face\n",
    "train_dataset = Dataset.from_dict({\"text\": train_texts.tolist(), \"label\": train_labels_encoded})\n",
    "test_dataset = Dataset.from_dict({\"text\": test_texts.tolist(), \"label\": test_labels_encoded})\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Загружаем модель DistilBERT\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(categories)).to(device)\n",
    "\n",
    "# Аргументы для тренировки\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    num_train_epochs= 0.1,  ####################### иначе работает 5 часов, было 3\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.05,   ################# иначе работает 5 часов. было 0,01\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Trainer API Hugging Face\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Обучаем модель\n",
    "trainer.train()\n",
    "\n",
    "# Делаем предсказания\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "pred_labels_text = label_encoder.inverse_transform(pred_labels)\n",
    "\n",
    "# Выводим метрики\n",
    "print(classification_report(test_labels, pred_labels_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8fcc365-d576-4d7b-9647-c6bee85f2ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение завершено, модель и векторизатор сохранены!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "\n",
    "# Заново создаем и обучаем векторизатор\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "# Создаем и обучаем SVM\n",
    "svm_model = SVC(kernel=\"linear\")\n",
    "svm_model.fit(X_train_tfidf, train_labels)\n",
    "\n",
    "# Сохраняем обученную модель и векторизатор\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "joblib.dump(svm_model, \"svm_model.pkl\")\n",
    "\n",
    "print(\"Обучение завершено, модель и векторизатор сохранены!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "83c4396b-41a1-45ee-b7ed-66cdb9d21c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM модель и TF-IDF векторизатор сохранены.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Сохранение модели SVM и векторизатора\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "joblib.dump(svm_model, \"svm_model.pkl\")\n",
    "print(\"SVM модель и TF-IDF векторизатор сохранены.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8dc516e5-1270-40d7-8c4a-06b4a461a47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM модель и TF-IDF векторизатор загружены.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(\"tfidf_vectorizer.pkl\") and os.path.exists(\"svm_model.pkl\"):\n",
    "    vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "    svm_model = joblib.load(\"svm_model.pkl\")\n",
    "    print(\"SVM модель и TF-IDF векторизатор загружены.\")\n",
    "else:\n",
    "    print(\"Ошибка: файлы моделей не найдены! Проверь, что они сохранены.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "610e3475-38a1-49f6-a46f-b63e7a2fe63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель DistilBERT сохранена!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"distilbert_bbc.pth\")\n",
    "print(\"Модель DistilBERT сохранена!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "57183175-e537-4094-84bd-b048e0955817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM и TF-IDF векторизатор сохранены!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "\n",
    "# Обучаем заново (если файлы потеряны)\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(train_texts)\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train_tfidf, train_labels)\n",
    "\n",
    "# Сохраняем\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "joblib.dump(svm_model, \"svm_model.pkl\")\n",
    "\n",
    "print(\"SVM и TF-IDF векторизатор сохранены!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "87a23a83-fb86-4467-adbd-94d8f4ec7be7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutException",
     "evalue": "Message: \nStacktrace:\n\tGetHandleVerifier [0x002BB5A3+24387]\n\t(No symbol) [0x00245904]\n\t(No symbol) [0x00120753]\n\t(No symbol) [0x00168BA9]\n\t(No symbol) [0x00168EFB]\n\t(No symbol) [0x001B19C2]\n\t(No symbol) [0x0018D894]\n\t(No symbol) [0x001AF138]\n\t(No symbol) [0x0018D646]\n\t(No symbol) [0x0015C59F]\n\t(No symbol) [0x0015D8E4]\n\tGetHandleVerifier [0x005BD883+3179043]\n\tGetHandleVerifier [0x005D6CF9+3282585]\n\tGetHandleVerifier [0x005D167C+3260444]\n\tGetHandleVerifier [0x00354330+650448]\n\t(No symbol) [0x0024ED0D]\n\t(No symbol) [0x0024BAF8]\n\t(No symbol) [0x0024BC99]\n\t(No symbol) [0x0023E530]\n\tBaseThreadInitThunk [0x756FFCC9+25]\n\tRtlGetAppContainerNamedObjectPath [0x778D82AE+286]\n\tRtlGetAppContainerNamedObjectPath [0x778D827E+238]\n\t(No symbol) [0x00000000]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m         driver\u001b[38;5;241m.\u001b[39mquit()  \u001b[38;5;66;03m# Закрываем браузер\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Проверяем\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m bbc_news \u001b[38;5;241m=\u001b[39m get_bbc_news()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBBC News Headlines:\u001b[39m\u001b[38;5;124m\"\u001b[39m, bbc_news)\n",
      "Cell \u001b[1;32mIn[91], line 15\u001b[0m, in \u001b[0;36mget_bbc_news\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.bbc.com/news\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Ждем появления заголовков (например, в div с классом \"gs-c-promo-heading__title\")\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m WebDriverWait(driver, \u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39muntil(\n\u001b[0;32m     16\u001b[0m     EC\u001b[38;5;241m.\u001b[39mpresence_of_element_located((By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.gs-c-promo-heading__title\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Парсим контент после загрузки\u001b[39;00m\n\u001b[0;32m     20\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(driver\u001b[38;5;241m.\u001b[39mpage_source, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:146\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll)\n\u001b[1;32m--> 146\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[1;31mTimeoutException\u001b[0m: Message: \nStacktrace:\n\tGetHandleVerifier [0x002BB5A3+24387]\n\t(No symbol) [0x00245904]\n\t(No symbol) [0x00120753]\n\t(No symbol) [0x00168BA9]\n\t(No symbol) [0x00168EFB]\n\t(No symbol) [0x001B19C2]\n\t(No symbol) [0x0018D894]\n\t(No symbol) [0x001AF138]\n\t(No symbol) [0x0018D646]\n\t(No symbol) [0x0015C59F]\n\t(No symbol) [0x0015D8E4]\n\tGetHandleVerifier [0x005BD883+3179043]\n\tGetHandleVerifier [0x005D6CF9+3282585]\n\tGetHandleVerifier [0x005D167C+3260444]\n\tGetHandleVerifier [0x00354330+650448]\n\t(No symbol) [0x0024ED0D]\n\t(No symbol) [0x0024BAF8]\n\t(No symbol) [0x0024BC99]\n\t(No symbol) [0x0023E530]\n\tBaseThreadInitThunk [0x756FFCC9+25]\n\tRtlGetAppContainerNamedObjectPath [0x778D82AE+286]\n\tRtlGetAppContainerNamedObjectPath [0x778D827E+238]\n\t(No symbol) [0x00000000]\n"
     ]
    }
   ],
   "source": [
    "def get_bbc_news():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Запуск без интерфейса браузера\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    # Инициализация драйвера\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    try:\n",
    "        driver.get(\"https://www.bbc.com/news\")\n",
    "\n",
    "        # Ждем появления заголовков (например, в div с классом \"gs-c-promo-heading__title\")\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \".gs-c-promo-heading__title\"))\n",
    "        )\n",
    "\n",
    "        # Парсим контент после загрузки\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Поиск заголовков\n",
    "        headlines = [h.text.strip() for h in soup.find_all(\"a\", class_=\"gs-c-promo-heading__title\") if h.text.strip()]\n",
    "        return headlines[:10]  # Берем 10 заголовков\n",
    "\n",
    "    finally:\n",
    "        driver.quit()  # Закрываем браузер\n",
    "\n",
    "# Проверяем\n",
    "bbc_news = get_bbc_news()\n",
    "print(\"BBC News Headlines:\", bbc_news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9f8385ff-d699-4a8a-8dc4-d26e629810c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 47\u001b[0m\n\u001b[0;32m     42\u001b[0m categories \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbusiness\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentertainment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolitics\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msport\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtech\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Запуск предсказаний\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchrome\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Service\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchrome\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Options\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import joblib\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "import numpy as np\n",
    "\n",
    "# Загрузка моделей\n",
    "vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "svm_model = joblib.load(\"svm_model.pkl\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "bert_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=5).to(device)\n",
    "bert_model.load_state_dict(torch.load(\"distilbert_bbc.pth\", map_location=device))\n",
    "bert_model.eval()\n",
    "\n",
    "# Функция парсинга заголовков с BBC\n",
    "def get_bbc_news():\n",
    "    url = \"https://www.bbc.com/news\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    headlines = [h.text.strip() for h in soup.find_all(\"h3\")][:10]  # Берем 10 заголовков\n",
    "    return headlines\n",
    "\n",
    "# Функция предсказания SVM\n",
    "def predict_svm(texts):\n",
    "    vectors = vectorizer.transform(texts)\n",
    "    predictions = svm_model.predict(vectors)\n",
    "    return predictions\n",
    "\n",
    "# Функция предсказания DistilBERT\n",
    "def predict_bert(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    return predictions\n",
    "\n",
    "# Классы новостей\n",
    "categories = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "\n",
    "# Запуск предсказаний\n",
    " \n",
    "\n",
    "bbc_news = get_bbc_news()\n",
    "print(\"\\nЗаголовки BBC:\", bbc_news)  # Проверяем, загружены ли новости\n",
    "\n",
    "if not bbc_news:\n",
    "    raise ValueError(\"Ошибка: не удалось получить заголовки новостей! Проверь разметку сайта.\")\n",
    "\n",
    "\n",
    "svm_preds = predict_svm(bbc_news)\n",
    "bert_preds = predict_bert(bbc_news)\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"\\n--- BBC News Classification ---\")\n",
    "for i, text in enumerate(bbc_news):\n",
    "    print(f\"\\nNews: {text}\")\n",
    "    print(f\"SVM Prediction: {categories[svm_preds[i]]}\")\n",
    "    print(f\"BERT Prediction: {categories[bert_preds[i]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "37256ecd-8e60-4882-a689-d95a1f7ac0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model.pkl']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Сохранение TF-IDF векторизатора\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "# Сохранение SVM модели\n",
    "joblib.dump(svm_model, \"svm_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3b226493-e697-4f2b-8ac0-3d195101fbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', '0.26.0', 'bbc', 'bbc.zip', 'distilbert_bbc.pth', 'HomeWork.pptx', 'logs', 'results', 'svm_model.pkl', 'tfidf_vectorizer.pkl', 'Untitled.ipynb', 'Untitled1.ipynb', 'Untitled2.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())  # Покажет файлы в текущей директории\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fc933759-db7d-43b2-b448-c91466811df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\epg_F\\Homework_6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())  # Покажет текущую рабочую папку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "db5a8379-f370-4799-820f-69da03b74474",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: './distilbert_model'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py:342\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 342\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[0;32m    343\u001b[0m         path_or_repo_id,\n\u001b[0;32m    344\u001b[0m         filename,\n\u001b[0;32m    345\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[0;32m    346\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m    347\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    348\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    349\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    350\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    351\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    352\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    353\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    354\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    355\u001b[0m     )\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 106\u001b[0m     validate_repo_id(arg_value)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './distilbert_model'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m distilbert_tokenizer \u001b[38;5;241m=\u001b[39m DistilBertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m distilbert_model \u001b[38;5;241m=\u001b[39m DistilBertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./distilbert_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_svm\u001b[39m(text):\n\u001b[0;32m     20\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Классификация новости с помощью SVM.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:3540\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3538\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m   3539\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[1;32m-> 3540\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m   3541\u001b[0m             pretrained_model_name_or_path,\n\u001b[0;32m   3542\u001b[0m             CONFIG_NAME,\n\u001b[0;32m   3543\u001b[0m             cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   3544\u001b[0m             force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   3545\u001b[0m             resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m   3546\u001b[0m             proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   3547\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   3548\u001b[0m             token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   3549\u001b[0m             revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   3550\u001b[0m             subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m   3551\u001b[0m             _raise_exceptions_for_gated_repo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   3552\u001b[0m             _raise_exceptions_for_missing_entries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   3553\u001b[0m             _raise_exceptions_for_connection_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   3554\u001b[0m         )\n\u001b[0;32m   3555\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m   3556\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py:408\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 408\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    410\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[1;31mOSError\u001b[0m: Incorrect path_or_model_id: './distilbert_model'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import torch\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Загрузка SVM модели и TF-IDF векторизатора\n",
    "vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "svm_model = joblib.load(\"svm_model.pkl\")\n",
    "\n",
    "# Загрузка DistilBERT модели и токенизатора\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "distilbert_model = DistilBertForSequenceClassification.from_pretrained(\"./distilbert_model\").to(device)\n",
    "\n",
    "def predict_svm(text):\n",
    "    \"\"\"Классификация новости с помощью SVM.\"\"\"\n",
    "    text_vectorized = vectorizer.transform([text])\n",
    "    prediction = svm_model.predict(text_vectorized)[0]\n",
    "    probabilities = svm_model.decision_function(text_vectorized)\n",
    "    probabilities = np.exp(probabilities) / np.sum(np.exp(probabilities))  # Softmax-вероятности\n",
    "    return prediction, probabilities\n",
    "\n",
    "def predict_distilbert(text):\n",
    "    \"\"\"Классификация новости с помощью DistilBERT.\"\"\"\n",
    "    inputs = distilbert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = distilbert_model(**inputs)\n",
    "    probabilities = F.softmax(outputs.logits, dim=-1).cpu().numpy()[0]\n",
    "    prediction = np.argmax(probabilities)\n",
    "    return prediction, probabilities\n",
    "\n",
    "def scrape_bbc_news():\n",
    "    \"\"\"Собираем свежие новости с BBC.\"\"\"\n",
    "    categories = {\n",
    "        \"business\": \"https://www.bbc.com/news/business\",\n",
    "        \"entertainment\": \"https://www.bbc.com/news/entertainment_and_arts\",\n",
    "        \"politics\": \"https://www.bbc.com/news/politics\",\n",
    "        \"sport\": \"https://www.bbc.com/sport\",\n",
    "        \"tech\": \"https://www.bbc.com/news/technology\"\n",
    "    }\n",
    "    news_data = []\n",
    "    \n",
    "    for category, url in categories.items():\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        articles = soup.find_all(\"a\", class_=\"gs-c-promo-heading\")[:5]  # Берем 5 новостей\n",
    "        \n",
    "        for article in articles:\n",
    "            title = article.text.strip()\n",
    "            link = \"https://www.bbc.com\" + article[\"href\"]\n",
    "            news_data.append((category, title, link))\n",
    "    \n",
    "    return news_data\n",
    "\n",
    "# Скрапим новости\n",
    "bbc_news = scrape_bbc_news()\n",
    "\n",
    "# Прогоняем через пайплайны\n",
    "for category, title, link in bbc_news:\n",
    "    svm_pred, svm_prob = predict_svm(title)\n",
    "    bert_pred, bert_prob = predict_distilbert(title)\n",
    "    print(f\"Category: {category}\")\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Link: {link}\")\n",
    "    print(f\"SVM Prediction: {svm_pred}, Probabilities: {svm_prob}\")\n",
    "    print(f\"DistilBERT Prediction: {bert_pred}, Probabilities: {bert_prob}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Пример использования\n",
    "text = \"The government has announced a new economic policy to boost business.\"\n",
    "print(\"SVM Prediction:\", predict_svm(text))\n",
    "print(\"DistilBERT Prediction:\", predict_distilbert(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416c9f42-2798-4df0-8666-fed34736ce03",
   "metadata": {},
   "source": [
    "### 2я часть\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7648347-9b3c-4c5b-bf31-46599810c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f377bf36-6642-4016-9d44-3dc6f35a5e77",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Compiled extensions are unavailable. If you've installed from a package, ask the package maintainer to include compiled extensions. If you're building Gensim from source yourself, install Cython and a C compiler, and then run `python setup.py build_ext --inplace` to retry. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\matutils.py:1356\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\_mmreader.pyx:11\u001b[0m, in \u001b[0;36minit gensim.corpora._mmreader\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name utils",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m corpora, models\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Загружаем английскую модель SpaCy для лемматизации\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\matutils.py:1358\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[118], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m corpora, models\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Загружаем английскую модель SpaCy для лемматизации\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\matutils.py:1358\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Compiled extensions are unavailable. If you've installed from a package, ask the package maintainer to include compiled extensions. If you're building Gensim from source yourself, install Cython and a C compiler, and then run `python setup.py build_ext --inplace` to retry. "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim import corpora, models\n",
    "from collections import defaultdict\n",
    "\n",
    "# Загружаем английскую модель SpaCy для лемматизации\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Путь к данным\n",
    "bbc_path = r\"C:\\Users\\epg_F\\Homework_6\\bbc\"\n",
    "\n",
    "# Функция для загрузки и предобработки текстов\n",
    "def load_and_preprocess_data(path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for category in os.listdir(path):\n",
    "        category_path = os.path.join(path, category)\n",
    "        if os.path.isdir(category_path):\n",
    "            for file_name in os.listdir(category_path):\n",
    "                file_path = os.path.join(category_path, file_name)\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "                    text = file.read().lower()\n",
    "                    text = re.sub(r'[^a-z\\s]', '', text)  # Удаление пунктуации\n",
    "                    doc = nlp(text)\n",
    "                    lemmatized_text = \" \".join([token.lemma_ for token in doc if not token.is_stop])\n",
    "                    data.append(lemmatized_text)\n",
    "                    labels.append(category)\n",
    "    return pd.DataFrame({\"text\": data, \"category\": labels})\n",
    "\n",
    "# Загружаем данные\n",
    "df = load_and_preprocess_data(bbc_path)\n",
    "\n",
    "# Функция для построения тематической модели LDA\n",
    "def train_lda(df, category, num_topics=5):\n",
    "    category_texts = df[df['category'] == category]['text'].tolist()\n",
    "    vectorizer = CountVectorizer()\n",
    "    doc_term_matrix = vectorizer.fit_transform(category_texts)\n",
    "    \n",
    "    # Преобразуем в формат gensim\n",
    "    dictionary = corpora.Dictionary([text.split() for text in category_texts])\n",
    "    corpus = [dictionary.doc2bow(text.split()) for text in category_texts]\n",
    "    \n",
    "    # Обучаем LDA\n",
    "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "    \n",
    "    return lda_model, corpus, dictionary\n",
    "\n",
    "# Визуализация wordcloud для тем\n",
    "def plot_wordcloud(lda_model, dictionary, num_topics):\n",
    "    for i in range(num_topics):\n",
    "        words = lda_model.show_topic(i, 20)\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(words))\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Тема {i+1}\")\n",
    "        plt.show()\n",
    "\n",
    "# Визуализация распределения тем\n",
    "def visualize_lda(lda_model, corpus, dictionary):\n",
    "    vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "    pyLDAvis.display(vis)\n",
    "\n",
    "# Запуск тематического моделирования для каждой категории\n",
    "categories = df['category'].unique()\n",
    "num_topics = 5  # Количество тем для каждой категории\n",
    "\n",
    "for category in categories:\n",
    "    print(f\"Обрабатываем категорию: {category}\")\n",
    "    lda_model, corpus, dictionary = train_lda(df, category, num_topics)\n",
    "    plot_wordcloud(lda_model, dictionary, num_topics)\n",
    "    visualize_lda(lda_model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f043fc-7d20-414d-bf80-d8ba51250c18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
