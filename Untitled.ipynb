{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32293ea4-f0cd-47a2-80f8-22a99f76cd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.99      0.98      0.99       101\n",
      "entertainment       1.00      0.97      0.99        77\n",
      "     politics       0.98      0.98      0.98        84\n",
      "        sport       1.00      1.00      1.00       103\n",
      "         tech       0.96      1.00      0.98        80\n",
      "\n",
      "     accuracy                           0.99       445\n",
      "    macro avg       0.99      0.99      0.99       445\n",
      " weighted avg       0.99      0.99      0.99       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–∞–Ω–Ω—ã–º–∏\n",
    "data_path = r\"C:\\Users\\epg_F\\Homework_6\\bbc\"\n",
    "\n",
    "# –°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–Ω–∞–∑–≤–∞–Ω–∏—è –ø–∞–ø–æ–∫)\n",
    "categories = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–∫–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–¥–∏—Ä–æ–≤–∫–∏\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    return result[\"encoding\"]\n",
    "\n",
    "# –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª—ã –∏–∑ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
    "for category in categories:\n",
    "    category_path = os.path.join(data_path, category)\n",
    "    for filename in os.listdir(category_path):\n",
    "        file_path = os.path.join(category_path, filename)\n",
    "        encoding = detect_encoding(file_path)  # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É\n",
    "        with open(file_path, \"r\", encoding=encoding, errors=\"replace\") as file:\n",
    "            texts.append(file.read())\n",
    "            labels.append(category)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º DataFrame\n",
    "df = pd.DataFrame({\"text\": texts, \"label\": labels})\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"text\"], df[\"label\"], test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z ]', '', text)  # –£–¥–∞–ª—è–µ–º –≤—Å–µ, –∫—Ä–æ–º–µ –±—É–∫–≤ –∏ –ø—Ä–æ–±–µ–ª–æ–≤\n",
    "    text = text.lower().strip()  # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    return text\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É\n",
    "train_texts = train_texts.apply(preprocess_text)\n",
    "test_texts = test_texts.apply(preprocess_text)\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ SVM-–º–æ–¥–µ–ª–∏ —Å TF-IDF\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç –≤ –≤–µ–∫—Ç–æ—Ä—ã\n",
    "    ('clf', SVC(probability=True))  # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º SVM\n",
    "])\n",
    "\n",
    "pipeline.fit(train_texts, train_labels)  # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "predictions = pipeline.predict(test_texts)  # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "print(classification_report(test_labels, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcb41620-a0dc-4983-a24d-30bc2cb88450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label\n",
      "0  Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
      "1  Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
      "2  Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
      "3  High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
      "4  Pernod takeover talk lifts Domecq\\n\\nShares in...  business\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "# –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–∞–Ω–Ω—ã–º–∏\n",
    "data_path = r\"C:\\Users\\epg_F\\Homework_6\\bbc\"\n",
    "\n",
    "# –°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–Ω–∞–∑–≤–∞–Ω–∏—è –ø–∞–ø–æ–∫)\n",
    "categories = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–∫–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–¥–∏—Ä–æ–≤–∫–∏\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    return result[\"encoding\"]\n",
    "\n",
    "# –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª—ã –∏–∑ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
    "for category in categories:\n",
    "    category_path = os.path.join(data_path, category)\n",
    "    for filename in os.listdir(category_path):\n",
    "        file_path = os.path.join(category_path, filename)\n",
    "        encoding = detect_encoding(file_path)  # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É\n",
    "        with open(file_path, \"r\", encoding=encoding, errors=\"replace\") as file:\n",
    "            texts.append(file.read())\n",
    "            labels.append(category)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º DataFrame\n",
    "df = pd.DataFrame({\"text\": texts, \"label\": labels})\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2f9fd83-0456-4040-8f6a-6f1e3508a177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"text\"], df[\"label\"], test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3b3acca-6610-4b74-97da-0fd32aa2dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z ]', '', text)  # –£–¥–∞–ª—è–µ–º –≤—Å–µ, –∫—Ä–æ–º–µ –±—É–∫–≤ –∏ –ø—Ä–æ–±–µ–ª–æ–≤\n",
    "    text = text.lower().strip()  # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    return text\n",
    "\n",
    "train_texts = train_texts.apply(preprocess_text)\n",
    "test_texts = test_texts.apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6de05e82-0f00-4f4f-a887-e97466a6a391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.99      0.98      0.99       101\n",
      "entertainment       1.00      0.97      0.99        77\n",
      "     politics       0.98      0.98      0.98        84\n",
      "        sport       1.00      1.00      1.00       103\n",
      "         tech       0.96      1.00      0.98        80\n",
      "\n",
      "     accuracy                           0.99       445\n",
      "    macro avg       0.99      0.99      0.99       445\n",
      " weighted avg       0.99      0.99      0.99       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç –≤ –≤–µ–∫—Ç–æ—Ä—ã\n",
    "    ('clf', SVC(probability=True))  # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º SVM\n",
    "])\n",
    "\n",
    "pipeline.fit(train_texts, train_labels)  # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "predictions = pipeline.predict(test_texts)  # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "\n",
    "print(classification_report(test_labels, predictions))  # –í—ã–≤–æ–¥–∏–º –º–µ—Ç—Ä–∏–∫–∏\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a3fac37-9c06-49e9-97d9-7af27b622c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32d72857e2d44c59697a3e505f737ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1776 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f05e0d3cd54eeb8180ec61bfc9fd6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/445 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\epg_F\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23/23 13:03, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.605500</td>\n",
       "      <td>1.594041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.38      0.33      0.35       101\n",
      "entertainment       0.06      0.04      0.05        77\n",
      "     politics       0.48      0.76      0.59        84\n",
      "        sport       0.23      0.39      0.29       103\n",
      "         tech       0.00      0.00      0.00        80\n",
      "\n",
      "     accuracy                           0.31       445\n",
      "    macro avg       0.23      0.30      0.25       445\n",
      " weighted avg       0.24      0.31      0.27       445\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\epg_F\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\epg_F\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\epg_F\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# –ö–æ–¥–∏—Ä—É–µ–º –ª–µ–π–±–ª—ã –≤ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã Hugging Face\n",
    "train_dataset = Dataset.from_dict({\"text\": train_texts.tolist(), \"label\": train_labels_encoded})\n",
    "test_dataset = Dataset.from_dict({\"text\": test_texts.tolist(), \"label\": test_labels_encoded})\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å DistilBERT\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(categories)).to(device)\n",
    "\n",
    "# –ê—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    num_train_epochs= 0.1,  ####################### –∏–Ω–∞—á–µ —Ä–∞–±–æ—Ç–∞–µ—Ç 5 —á–∞—Å–æ–≤, –±—ã–ª–æ 3\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.05,   ################# –∏–Ω–∞—á–µ —Ä–∞–±–æ—Ç–∞–µ—Ç 5 —á–∞—Å–æ–≤. –±—ã–ª–æ 0,01\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Trainer API Hugging Face\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "trainer.train()\n",
    "\n",
    "# –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "pred_labels_text = label_encoder.inverse_transform(pred_labels)\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º –º–µ—Ç—Ä–∏–∫–∏\n",
    "print(classification_report(test_labels, pred_labels_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8fcc365-d576-4d7b-9647-c6bee85f2ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ, –º–æ–¥–µ–ª—å –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "\n",
    "# –ó–∞–Ω–æ–≤–æ —Å–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º SVM\n",
    "svm_model = SVC(kernel=\"linear\")\n",
    "svm_model.fit(X_train_tfidf, train_labels)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "joblib.dump(svm_model, \"svm_model.pkl\")\n",
    "\n",
    "print(\"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ, –º–æ–¥–µ–ª—å –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "83c4396b-41a1-45ee-b7ed-66cdb9d21c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM –º–æ–¥–µ–ª—å –∏ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ SVM –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "joblib.dump(svm_model, \"svm_model.pkl\")\n",
    "print(\"SVM –º–æ–¥–µ–ª—å –∏ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8dc516e5-1270-40d7-8c4a-06b4a461a47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM –º–æ–¥–µ–ª—å –∏ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(\"tfidf_vectorizer.pkl\") and os.path.exists(\"svm_model.pkl\"):\n",
    "    vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "    svm_model = joblib.load(\"svm_model.pkl\")\n",
    "    print(\"SVM –º–æ–¥–µ–ª—å –∏ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã.\")\n",
    "else:\n",
    "    print(\"–û—à–∏–±–∫–∞: —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω—ã! –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –æ–Ω–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "610e3475-38a1-49f6-a46f-b63e7a2fe63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ú–æ–¥–µ–ª—å DistilBERT —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"distilbert_bbc.pth\")\n",
    "print(\"–ú–æ–¥–µ–ª—å DistilBERT —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "57183175-e537-4094-84bd-b048e0955817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM –∏ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "\n",
    "# –û–±—É—á–∞–µ–º –∑–∞–Ω–æ–≤–æ (–µ—Å–ª–∏ —Ñ–∞–π–ª—ã –ø–æ—Ç–µ—Ä—è–Ω—ã)\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(train_texts)\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train_tfidf, train_labels)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "joblib.dump(svm_model, \"svm_model.pkl\")\n",
    "\n",
    "print(\"SVM –∏ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "87a23a83-fb86-4467-adbd-94d8f4ec7be7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutException",
     "evalue": "Message: \nStacktrace:\n\tGetHandleVerifier [0x002BB5A3+24387]\n\t(No symbol) [0x00245904]\n\t(No symbol) [0x00120753]\n\t(No symbol) [0x00168BA9]\n\t(No symbol) [0x00168EFB]\n\t(No symbol) [0x001B19C2]\n\t(No symbol) [0x0018D894]\n\t(No symbol) [0x001AF138]\n\t(No symbol) [0x0018D646]\n\t(No symbol) [0x0015C59F]\n\t(No symbol) [0x0015D8E4]\n\tGetHandleVerifier [0x005BD883+3179043]\n\tGetHandleVerifier [0x005D6CF9+3282585]\n\tGetHandleVerifier [0x005D167C+3260444]\n\tGetHandleVerifier [0x00354330+650448]\n\t(No symbol) [0x0024ED0D]\n\t(No symbol) [0x0024BAF8]\n\t(No symbol) [0x0024BC99]\n\t(No symbol) [0x0023E530]\n\tBaseThreadInitThunk [0x756FFCC9+25]\n\tRtlGetAppContainerNamedObjectPath [0x778D82AE+286]\n\tRtlGetAppContainerNamedObjectPath [0x778D827E+238]\n\t(No symbol) [0x00000000]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m         driver\u001b[38;5;241m.\u001b[39mquit()  \u001b[38;5;66;03m# –ó–∞–∫—Ä—ã–≤–∞–µ–º –±—Ä–∞—É–∑–µ—Ä\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# –ü—Ä–æ–≤–µ—Ä—è–µ–º\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m bbc_news \u001b[38;5;241m=\u001b[39m get_bbc_news()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBBC News Headlines:\u001b[39m\u001b[38;5;124m\"\u001b[39m, bbc_news)\n",
      "Cell \u001b[1;32mIn[91], line 15\u001b[0m, in \u001b[0;36mget_bbc_news\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.bbc.com/news\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# –ñ–¥–µ–º –ø–æ—è–≤–ª–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ div —Å –∫–ª–∞—Å—Å–æ–º \"gs-c-promo-heading__title\")\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m WebDriverWait(driver, \u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39muntil(\n\u001b[0;32m     16\u001b[0m     EC\u001b[38;5;241m.\u001b[39mpresence_of_element_located((By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.gs-c-promo-heading__title\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# –ü–∞—Ä—Å–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏\u001b[39;00m\n\u001b[0;32m     20\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(driver\u001b[38;5;241m.\u001b[39mpage_source, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:146\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll)\n\u001b[1;32m--> 146\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[1;31mTimeoutException\u001b[0m: Message: \nStacktrace:\n\tGetHandleVerifier [0x002BB5A3+24387]\n\t(No symbol) [0x00245904]\n\t(No symbol) [0x00120753]\n\t(No symbol) [0x00168BA9]\n\t(No symbol) [0x00168EFB]\n\t(No symbol) [0x001B19C2]\n\t(No symbol) [0x0018D894]\n\t(No symbol) [0x001AF138]\n\t(No symbol) [0x0018D646]\n\t(No symbol) [0x0015C59F]\n\t(No symbol) [0x0015D8E4]\n\tGetHandleVerifier [0x005BD883+3179043]\n\tGetHandleVerifier [0x005D6CF9+3282585]\n\tGetHandleVerifier [0x005D167C+3260444]\n\tGetHandleVerifier [0x00354330+650448]\n\t(No symbol) [0x0024ED0D]\n\t(No symbol) [0x0024BAF8]\n\t(No symbol) [0x0024BC99]\n\t(No symbol) [0x0023E530]\n\tBaseThreadInitThunk [0x756FFCC9+25]\n\tRtlGetAppContainerNamedObjectPath [0x778D82AE+286]\n\tRtlGetAppContainerNamedObjectPath [0x778D827E+238]\n\t(No symbol) [0x00000000]\n"
     ]
    }
   ],
   "source": [
    "def get_bbc_news():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # –ó–∞–ø—É—Å–∫ –±–µ–∑ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –±—Ä–∞—É–∑–µ—Ä–∞\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥—Ä–∞–π–≤–µ—Ä–∞\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    try:\n",
    "        driver.get(\"https://www.bbc.com/news\")\n",
    "\n",
    "        # –ñ–¥–µ–º –ø–æ—è–≤–ª–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ div —Å –∫–ª–∞—Å—Å–æ–º \"gs-c-promo-heading__title\")\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \".gs-c-promo-heading__title\"))\n",
    "        )\n",
    "\n",
    "        # –ü–∞—Ä—Å–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # –ü–æ–∏—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤\n",
    "        headlines = [h.text.strip() for h in soup.find_all(\"a\", class_=\"gs-c-promo-heading__title\") if h.text.strip()]\n",
    "        return headlines[:10]  # –ë–µ—Ä–µ–º 10 –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤\n",
    "\n",
    "    finally:\n",
    "        driver.quit()  # –ó–∞–∫—Ä—ã–≤–∞–µ–º –±—Ä–∞—É–∑–µ—Ä\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º\n",
    "bbc_news = get_bbc_news()\n",
    "print(\"BBC News Headlines:\", bbc_news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9f8385ff-d699-4a8a-8dc4-d26e629810c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 47\u001b[0m\n\u001b[0;32m     42\u001b[0m categories \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbusiness\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentertainment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolitics\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msport\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtech\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# –ó–∞–ø—É—Å–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchrome\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Service\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchrome\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Options\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import joblib\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "import numpy as np\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π\n",
    "vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "svm_model = joblib.load(\"svm_model.pkl\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "bert_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=5).to(device)\n",
    "bert_model.load_state_dict(torch.load(\"distilbert_bbc.pth\", map_location=device))\n",
    "bert_model.eval()\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å BBC\n",
    "def get_bbc_news():\n",
    "    url = \"https://www.bbc.com/news\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    headlines = [h.text.strip() for h in soup.find_all(\"h3\")][:10]  # –ë–µ—Ä–µ–º 10 –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤\n",
    "    return headlines\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è SVM\n",
    "def predict_svm(texts):\n",
    "    vectors = vectorizer.transform(texts)\n",
    "    predictions = svm_model.predict(vectors)\n",
    "    return predictions\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è DistilBERT\n",
    "def predict_bert(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    return predictions\n",
    "\n",
    "# –ö–ª–∞—Å—Å—ã –Ω–æ–≤–æ—Å—Ç–µ–π\n",
    "categories = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    " \n",
    "\n",
    "bbc_news = get_bbc_news()\n",
    "print(\"\\n–ó–∞–≥–æ–ª–æ–≤–∫–∏ BBC:\", bbc_news)  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≥—Ä—É–∂–µ–Ω—ã –ª–∏ –Ω–æ–≤–æ—Å—Ç–∏\n",
    "\n",
    "if not bbc_news:\n",
    "    raise ValueError(\"–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π! –ü—Ä–æ–≤–µ—Ä—å —Ä–∞–∑–º–µ—Ç–∫—É —Å–∞–π—Ç–∞.\")\n",
    "\n",
    "\n",
    "svm_preds = predict_svm(bbc_news)\n",
    "bert_preds = predict_bert(bbc_news)\n",
    "\n",
    "# –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "print(\"\\n--- BBC News Classification ---\")\n",
    "for i, text in enumerate(bbc_news):\n",
    "    print(f\"\\nNews: {text}\")\n",
    "    print(f\"SVM Prediction: {categories[svm_preds[i]]}\")\n",
    "    print(f\"BERT Prediction: {categories[bert_preds[i]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "37256ecd-8e60-4882-a689-d95a1f7ac0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model.pkl']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ SVM –º–æ–¥–µ–ª–∏\n",
    "joblib.dump(svm_model, \"svm_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3b226493-e697-4f2b-8ac0-3d195101fbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', '0.26.0', 'bbc', 'bbc.zip', 'distilbert_bbc.pth', 'HomeWork.pptx', 'logs', 'results', 'svm_model.pkl', 'tfidf_vectorizer.pkl', 'Untitled.ipynb', 'Untitled1.ipynb', 'Untitled2.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())  # –ü–æ–∫–∞–∂–µ—Ç —Ñ–∞–π–ª—ã –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fc933759-db7d-43b2-b448-c91466811df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\epg_F\\Homework_6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())  # –ü–æ–∫–∞–∂–µ—Ç —Ç–µ–∫—É—â—É—é —Ä–∞–±–æ—á—É—é –ø–∞–ø–∫—É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "db5a8379-f370-4799-820f-69da03b74474",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: './distilbert_model'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py:342\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 342\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[0;32m    343\u001b[0m         path_or_repo_id,\n\u001b[0;32m    344\u001b[0m         filename,\n\u001b[0;32m    345\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[0;32m    346\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m    347\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    348\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    349\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    350\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    351\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    352\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    353\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    354\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    355\u001b[0m     )\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 106\u001b[0m     validate_repo_id(arg_value)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './distilbert_model'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m distilbert_tokenizer \u001b[38;5;241m=\u001b[39m DistilBertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m distilbert_model \u001b[38;5;241m=\u001b[39m DistilBertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./distilbert_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_svm\u001b[39m(text):\n\u001b[0;32m     20\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é SVM.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:3540\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3538\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m   3539\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[1;32m-> 3540\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m   3541\u001b[0m             pretrained_model_name_or_path,\n\u001b[0;32m   3542\u001b[0m             CONFIG_NAME,\n\u001b[0;32m   3543\u001b[0m             cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   3544\u001b[0m             force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   3545\u001b[0m             resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m   3546\u001b[0m             proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   3547\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   3548\u001b[0m             token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   3549\u001b[0m             revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   3550\u001b[0m             subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m   3551\u001b[0m             _raise_exceptions_for_gated_repo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   3552\u001b[0m             _raise_exceptions_for_missing_entries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   3553\u001b[0m             _raise_exceptions_for_connection_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   3554\u001b[0m         )\n\u001b[0;32m   3555\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m   3556\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\hub.py:408\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 408\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    410\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[1;31mOSError\u001b[0m: Incorrect path_or_model_id: './distilbert_model'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import torch\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ SVM –º–æ–¥–µ–ª–∏ –∏ TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞\n",
    "vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "svm_model = joblib.load(\"svm_model.pkl\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ DistilBERT –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "distilbert_model = DistilBertForSequenceClassification.from_pretrained(\"./distilbert_model\").to(device)\n",
    "\n",
    "def predict_svm(text):\n",
    "    \"\"\"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é SVM.\"\"\"\n",
    "    text_vectorized = vectorizer.transform([text])\n",
    "    prediction = svm_model.predict(text_vectorized)[0]\n",
    "    probabilities = svm_model.decision_function(text_vectorized)\n",
    "    probabilities = np.exp(probabilities) / np.sum(np.exp(probabilities))  # Softmax-–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n",
    "    return prediction, probabilities\n",
    "\n",
    "def predict_distilbert(text):\n",
    "    \"\"\"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é DistilBERT.\"\"\"\n",
    "    inputs = distilbert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = distilbert_model(**inputs)\n",
    "    probabilities = F.softmax(outputs.logits, dim=-1).cpu().numpy()[0]\n",
    "    prediction = np.argmax(probabilities)\n",
    "    return prediction, probabilities\n",
    "\n",
    "def scrape_bbc_news():\n",
    "    \"\"\"–°–æ–±–∏—Ä–∞–µ–º —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å BBC.\"\"\"\n",
    "    categories = {\n",
    "        \"business\": \"https://www.bbc.com/news/business\",\n",
    "        \"entertainment\": \"https://www.bbc.com/news/entertainment_and_arts\",\n",
    "        \"politics\": \"https://www.bbc.com/news/politics\",\n",
    "        \"sport\": \"https://www.bbc.com/sport\",\n",
    "        \"tech\": \"https://www.bbc.com/news/technology\"\n",
    "    }\n",
    "    news_data = []\n",
    "    \n",
    "    for category, url in categories.items():\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        articles = soup.find_all(\"a\", class_=\"gs-c-promo-heading\")[:5]  # –ë–µ—Ä–µ–º 5 –Ω–æ–≤–æ—Å—Ç–µ–π\n",
    "        \n",
    "        for article in articles:\n",
    "            title = article.text.strip()\n",
    "            link = \"https://www.bbc.com\" + article[\"href\"]\n",
    "            news_data.append((category, title, link))\n",
    "    \n",
    "    return news_data\n",
    "\n",
    "# –°–∫—Ä–∞–ø–∏–º –Ω–æ–≤–æ—Å—Ç–∏\n",
    "bbc_news = scrape_bbc_news()\n",
    "\n",
    "# –ü—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ –ø–∞–π–ø–ª–∞–π–Ω—ã\n",
    "for category, title, link in bbc_news:\n",
    "    svm_pred, svm_prob = predict_svm(title)\n",
    "    bert_pred, bert_prob = predict_distilbert(title)\n",
    "    print(f\"Category: {category}\")\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Link: {link}\")\n",
    "    print(f\"SVM Prediction: {svm_pred}, Probabilities: {svm_prob}\")\n",
    "    print(f\"DistilBERT Prediction: {bert_pred}, Probabilities: {bert_prob}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "text = \"The government has announced a new economic policy to boost business.\"\n",
    "print(\"SVM Prediction:\", predict_svm(text))\n",
    "print(\"DistilBERT Prediction:\", predict_distilbert(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416c9f42-2798-4df0-8666-fed34736ce03",
   "metadata": {},
   "source": [
    "### 2—è —á–∞—Å—Ç—å\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7648347-9b3c-4c5b-bf31-46599810c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f377bf36-6642-4016-9d44-3dc6f35a5e77",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Compiled extensions are unavailable. If you've installed from a package, ask the package maintainer to include compiled extensions. If you're building Gensim from source yourself, install Cython and a C compiler, and then run `python setup.py build_ext --inplace` to retry. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\matutils.py:1356\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\_mmreader.pyx:11\u001b[0m, in \u001b[0;36minit gensim.corpora._mmreader\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name utils",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m corpora, models\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# –ó–∞–≥—Ä—É–∂–∞–µ–º –∞–Ω–≥–ª–∏–π—Å–∫—É—é –º–æ–¥–µ–ª—å SpaCy –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\matutils.py:1358\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[118], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m corpora, models\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# –ó–∞–≥—Ä—É–∂–∞–µ–º –∞–Ω–≥–ª–∏–π—Å–∫—É—é –º–æ–¥–µ–ª—å SpaCy –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\matutils.py:1358\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Compiled extensions are unavailable. If you've installed from a package, ask the package maintainer to include compiled extensions. If you're building Gensim from source yourself, install Cython and a C compiler, and then run `python setup.py build_ext --inplace` to retry. "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim import corpora, models\n",
    "from collections import defaultdict\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –∞–Ω–≥–ª–∏–π—Å–∫—É—é –º–æ–¥–µ–ª—å SpaCy –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º\n",
    "bbc_path = r\"C:\\Users\\epg_F\\Homework_6\\bbc\"\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤\n",
    "def load_and_preprocess_data(path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for category in os.listdir(path):\n",
    "        category_path = os.path.join(path, category)\n",
    "        if os.path.isdir(category_path):\n",
    "            for file_name in os.listdir(category_path):\n",
    "                file_path = os.path.join(category_path, file_name)\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "                    text = file.read().lower()\n",
    "                    text = re.sub(r'[^a-z\\s]', '', text)  # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏\n",
    "                    doc = nlp(text)\n",
    "                    lemmatized_text = \" \".join([token.lemma_ for token in doc if not token.is_stop])\n",
    "                    data.append(lemmatized_text)\n",
    "                    labels.append(category)\n",
    "    return pd.DataFrame({\"text\": data, \"category\": labels})\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "df = load_and_preprocess_data(bbc_path)\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏ LDA\n",
    "def train_lda(df, category, num_topics=5):\n",
    "    category_texts = df[df['category'] == category]['text'].tolist()\n",
    "    vectorizer = CountVectorizer()\n",
    "    doc_term_matrix = vectorizer.fit_transform(category_texts)\n",
    "    \n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç gensim\n",
    "    dictionary = corpora.Dictionary([text.split() for text in category_texts])\n",
    "    corpus = [dictionary.doc2bow(text.split()) for text in category_texts]\n",
    "    \n",
    "    # –û–±—É—á–∞–µ–º LDA\n",
    "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "    \n",
    "    return lda_model, corpus, dictionary\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è wordcloud –¥–ª—è —Ç–µ–º\n",
    "def plot_wordcloud(lda_model, dictionary, num_topics):\n",
    "    for i in range(num_topics):\n",
    "        words = lda_model.show_topic(i, 20)\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(words))\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"–¢–µ–º–∞ {i+1}\")\n",
    "        plt.show()\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º\n",
    "def visualize_lda(lda_model, corpus, dictionary):\n",
    "    vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "    pyLDAvis.display(vis)\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
    "categories = df['category'].unique()\n",
    "num_topics = 5  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
    "\n",
    "for category in categories:\n",
    "    print(f\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é: {category}\")\n",
    "    lda_model, corpus, dictionary = train_lda(df, category, num_topics)\n",
    "    plot_wordcloud(lda_model, dictionary, num_topics)\n",
    "    visualize_lda(lda_model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f043fc-7d20-414d-bf80-d8ba51250c18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
